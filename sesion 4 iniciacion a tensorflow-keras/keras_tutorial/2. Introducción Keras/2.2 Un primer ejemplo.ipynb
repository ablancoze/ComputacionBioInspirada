{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Primer ejemplo con Keras + TF + Python\n",
    "\n",
    "Este es nuestro primer [Jupyter Notebook](http://jupyter.org) de Deep Learning con [Keras](https://keras.io) (y Python usando de fondo TensorFlow). \n",
    "\n",
    "<img src=\"./imgs/env.png\" width=500px/>\n",
    "\n",
    "Construiremos un ejemplo de red neuronal muy simple, con apenas un par de capas, para abordar el famoso problema [MNIST](https://en.wikipedia.org/wiki/MNIST_database). Nuestro objetivo es ver el flujo de trabajo habitual para describir un modelo en Keras, pero sin profundizar en los detalles, que veremos en temas posteriores.\n",
    "\n",
    "Además, también usaremos este notebook para familiarizarnos con el uso de Jupyter como entorno de desarrollo y experimentación. Para ejecutar los diversos chunks (así se llaman las únidades de código que conforman un notebook) de este notebook puedes pulsar en el botón *Run* (que hay a la izquierda de cada uno de ellos), o bien situando el cursor dentro del chunk y presionando la comnbinación de teclas *Shift+Enter*. También tienes la opción de usar las opciones de ejecución en el menú *Run* de la barra del navegador Jupyter. Ten en cuenta que algunas opciones pueden cambiar dependiendo del entorno en el que estés trabajando con este notebook (Jupyter Notebook, Jupyter Lab, o Collab de Google.. cualquiera de los tres funcionan de forma similar, pero con ligeras diferencias). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El primer paso es cargar la librería keras que permitirá interactuar a Python con la librería de Deep Learning que usemos (en nuestro caso, [Tensorflow](https://www.tensorflow.org)). \n",
    "\n",
    "En el chunk también mostramos la versión de keras que estamos usando e información de los dispositivos que usará Tensorflow para los cálculos siguientes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.4\n",
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 3884390114057302233\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "print(keras.__version__)\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparación de los datos\n",
    "\n",
    "El primer paso de todo modelado es la preparación y carga de los datos. En nuestro caso, del problema [MNIST](http://yann.lecun.com/exdb/mnist/), que consta de una gran base de datos de dígitos escritos y que es tan habitual que se ha convertido en un ejemplo paradigmático dentro de Machine Learning.\n",
    "\n",
    "El trabajo de preprocesamiento necesario para poder aplicar un modelo a este problema no es menor pero, afortunadamente, Keras proporciona una instrucción directa para descargar las imágenes que representan los miles de dígitos escritos a mano (ya con formato unificado de 28x28 pixels en escala de grises).\n",
    "\n",
    "Para poder cargas los datos que trae de ejemplo Keras hay que seguir dos pasos: primero, cargar la librería de Keras que prporciona las herramientas para trabajar con el dataset concreto (que suelen estár en el paquete `keras.datasets`, en este caso llamado `mnist`); y, segundo, ejecutar el proceso de carga de los datos (la librería proporciona la función `load_data()`). Ha de tenerse en cuenta que la primera vez que se realiza este proceso los datos se descargan desde un repositorio que viene por defecto predefinido en ese paquete, ya que, debido a su tamaño, no se instalan junto con la librería, sino únicamente cuando el usuario los necesita:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observa que el proceso de carga de datos separa adecuadamente las diversas partes de que consta este dataset:  (_conjunto de entrenamiento_, _conjunto de test_), y cada uno de estos conjuntos está formado por un conjunto de datos (imágenes, en este caso concreto), con sus respectivas etiquetas de clasificación (_labels_). Además, aprovechamos la capacidad sintáctica de Python para realizar la carga de todos estos conjuntos en un solo paso (haciendo una \n",
    "asignación múltiple).\n",
    "\n",
    "Podemos explorar un poco cómo son cada una de estas variables haciendo uso de instrucciones específicas de Python que nos dan información acerca de su estructura y muestra los primeros valores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, ..., 5, 6, 8], dtype=uint8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 2, 1, ..., 4, 5, 6], dtype=uint8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si queremos ver alguna de las imágenes que hay en el dataset, podemos hacer uso de la instrucción adecuada de, por ejemplo, la librería `matplotlib`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def gen_image(arr):\n",
    "    conv = (np.reshape(arr, (28, 28)) * 255).astype(np.uint8)\n",
    "    plt.imshow(conv, interpolation='nearest')\n",
    "    return plt\n",
    "\n",
    "gen_image(test_data[0]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El flujo de trabajo es similar al que se sigue siempre en los procesos de ML Supervisado, y que hemos analizado en el tema anterior: \n",
    "  1. Mostramos al modelo (una red neuronal, en nuestro caso) los datos de *entrenamiento*, `train_data` y `train_labels`.\n",
    "  2. El modelo debe *aprender* a asociar las imágenes con las etiquetas asociadas.\n",
    "  3. Por último, verificamos el aprendizaje realizado comprobando sobre `test_data` que las respuestas dadas por el modelo (*predicciones*) coinciden con las almacenadas en `test_labels`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definición del modelo\n",
    "\n",
    "Ya estamos en condiciones de definir una red neuronal que consumirá los datos anteriores para ver si somos capaces de dar una primera solución al problema del reconocimiento de dígitos manuscritos. Como solo estamos haciendo una primera aproximación a Keras, la red definida será muy básica, con solo una capa de entrada y una de salida:\n",
    "\n",
    "  1. Vamos a situar una capa de entrada con 784 (= 28 * 28) neuronas (que recibirán cada uno de los 784 pixels de cada imagen), con función de activación ReLU, y \n",
    "  2. una capa de salida con 10 neuronas (una neurona para cada una de las posibles etiquetas de salida), y con activación softmax (por lo que se podrá interpretar como una probabilidad de salida que indica lo probable que es que la imagen de entrada tenga cada una de las etiquetas como salida):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Failed to import `pydot`. Please install `pydot`. For example with `pip install pydot`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-ffda8f71ae35>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m512\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m28\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m28\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'softmax'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mplot_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'model_plot.png'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_layer_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\utils\\vis_utils.py\u001b[0m in \u001b[0;36mplot_model\u001b[1;34m(model, to_file, show_shapes, show_layer_names, rankdir)\u001b[0m\n\u001b[0;32m    130\u001b[0m             \u001b[1;34m'LR'\u001b[0m \u001b[0mcreates\u001b[0m \u001b[0ma\u001b[0m \u001b[0mhorizontal\u001b[0m \u001b[0mplot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m     \"\"\"\n\u001b[1;32m--> 132\u001b[1;33m     \u001b[0mdot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_to_dot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_shapes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_layer_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrankdir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    133\u001b[0m     \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextension\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplitext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mto_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mextension\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\utils\\vis_utils.py\u001b[0m in \u001b[0;36mmodel_to_dot\u001b[1;34m(model, show_shapes, show_layer_names, rankdir)\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m     \u001b[0m_check_pydot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m     \u001b[0mdot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpydot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[0mdot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'rankdir'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrankdir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\utils\\vis_utils.py\u001b[0m in \u001b[0;36m_check_pydot\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mpydot\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         raise ImportError(\n\u001b[1;32m---> 20\u001b[1;33m             \u001b[1;34m'Failed to import `pydot`. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m             \u001b[1;34m'Please install `pydot`. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m             'For example with `pip install pydot`.')\n",
      "\u001b[1;31mImportError\u001b[0m: Failed to import `pydot`. Please install `pydot`. For example with `pip install pydot`."
     ]
    }
   ],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "red = models.Sequential()\n",
    "red.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,)))\n",
    "red.add(layers.Dense(10, activation='softmax'))\n",
    "plot_model(red, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./imgs/model_plot.png\" />\n",
    "\n",
    "Para facilitar la comprensión de esta introducción, hemos hecho uso de las utilidades de Keras para dar una representación visual de la estructura de la red. Ha de tenerse en cuenta que para ello es necesario instalar [Graphviz](http://www.graphviz.org) y la librería `pydot` de Python que se comunica con ella.\n",
    "\n",
    "Además de las neuronas, que son las unidades atómicas que componen una red neuronal, desde un punto de vista funcional, el elemento básico de las redes neuronales es lo que se conoce como *capa* (*layer*), un módulo de procesamiento  formado por un conjunto de neuronas iguales que puede ver interpretarse como un \"filtro\" de datos. Como veremos a lo largo del curso, las capas son las encargadas de generar *representaciones* útiles de los datos que reciben, y que ayuden a resolver el problema para el que se ha construido la red. La mayor parte del Deep Learning, y donde esta demostrando un valor añadido respecto de los otros modelos de ML existentes, consiste en concatenar capas simples (y, posiblemente, con funcionalidades específicas diferenciadas) para obtener un dispositivo de cálculo que procesa datos de forma progresiva.\n",
    "\n",
    "En el caso de la red que hemos definido, este dispositivo consta de una secuencia de dos capas densas, que son capas neurales totalmente conectadas. La segunda (y última) capa es una capa \"softmax\" de 10 salidas, lo que significa que devolverá un vector probabilístico de 10 valores (es decir, 10 valores en $[0,1]$ que suman 1). Cada uno de estos valores se interpretará la probabilidad de que la imagen actual pertenezca a una de las 10 clases (los dígitos del 0 al 9).\n",
    "\n",
    "Hasta ahora solo hemos definido la estructura de la red, pero no hemos dado ninguna información acerca de cómo se llevará a cabo el entrenamiento. Para ello, hemos de indicarle a Keras algunas características adicionales, tales como el optimizador que permitirá modificar los pesos de la red, qué función objetivo (de error) se usará para dirigir esta optimización, y la métrica que usaremos para medir cómo se va comportando la red a medida que se entrena. \n",
    "\n",
    "Keras proporciona la función `compile` que permite establecer estas (y otras) propiedades sobre una red ya definida:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "red.compile(optimizer='rmsprop',\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Como se puede observar, muchos de los chunks no proporcionan una salida imprimible, sino que crean nuevas variables o modifican el contenido de algunas de ellas para su reutilización posterior.\n",
    "\n",
    "Debido a que la red neuronal que vamos a usar debe recibir como dato de entrada cada imagen de forma aplanada (es decir, no como una matriz de 28x28, sino como un vector de 28x28=784 posiciones), nuestro primer paso es hacer uso de las instrucciones que proporciona Keras para transformar la forma de los datos de entrada. Además, aprovecharemos para normalizar el contenido de estas imágenes (están en escalas de grises con valores `uint8` entre 0 y 255, y las pasaremos a valores `float32` en $[0,1]$), algo aconsejable cuando se trabaja con este tipo de modelos:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.reshape((60000, 28 * 28))\n",
    "train_data = train_data.astype('float32') / 255\n",
    "\n",
    "test_data = test_data.reshape((10000, 28 * 28))\n",
    "test_data = test_data.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Además, vamos a convertir las etiquetas (que vienen en el dataset como valores enteros), en vectores binarios para que se correspondan con la salida que nuestra red puede proporcionar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proceso de entrenamiento\n",
    "\n",
    "Preparados los datos y definida la red (estructura y funcionalidad), podemos hacer uso de la instrucción `fit` para comenzar el proceso de entrenamiento sobre los datos que tenemos. Esencialmente, hemos de indicar sobre qué datos entrenar (entrada y salidas), cuántas iteraciones (epochs) y con qué tamaño de batch (cada cuántos ejemplos el algoritmos actualiza los pesos).\n",
    "\n",
    "Durante el proceso de entrenamiento, Keras informa de los valores que toma la función objetivo, así como de la/s métrica/s que hemos fijado en la compilación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 7s 123us/step - loss: 0.2575 - acc: 0.9255\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 7s 113us/step - loss: 0.1035 - acc: 0.9694\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 7s 117us/step - loss: 0.0684 - acc: 0.9794\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 7s 117us/step - loss: 0.0489 - acc: 0.9852\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 7s 116us/step - loss: 0.0372 - acc: 0.9887\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e5d33f85c0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "red.fit(train_data, train_labels, epochs=5, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debemos tener en cuenta que los valores mostrados son el error y métricas calculados sobre los propios datos de entrenamiento. Sin embargo, como el objetivo de un modelo de aprendizaje es generalizar bien sobre datos que el proceso de entrenamiento no ha visto anteriormente, necesitamos el conjunto de test para evaluar cómo se comporta la red sobre ejemplos que no ha usado para ajustarse.\n",
    "\n",
    "Sobre los datos de entrenamiento alcanzamos rápidamente una precisión de 0.989 (i.e. 98.9%), pero veamos cómo de bien se comporta con los datos de test (que no ha usado para aprender):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 66us/step\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = red.evaluate(test_data, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_acc: 0.9782\n"
     ]
    }
   ],
   "source": [
    "print('test_acc:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Lo normal es que la red se comporte peor en los datos de test que en los datos de entrenamiento, ya que el proceso de entrenamiento consiste precisamente en ajustar los pesos para que el error cometido en estos últimos se minimice. Esta diferencia de comportamiento entre entrenamiento y test se denomina **overfitting** (o **sobreajuste**). En todo caso, con una red tan simple como la que hemos usado se alcanzan cotas de casi el 98% de aciertos.\n",
    "\n",
    "Finalmente, podemos ver las predicciones que hace la red sobre algunos datos del conjunto de test (mostramos también las etiquetas aaociadas a los datos usados, pero ten en cuenta que están en formato binarizado, y el índice 1 corresponde a la etiqueta 0, el índice 2 a la etiqueta 1, etc...):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(red.predict(test_data[2:3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels[2:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
